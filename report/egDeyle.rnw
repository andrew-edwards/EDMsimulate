% egDeyle.rnw - do manual calculations for simplex() and then s_map() following
%  the descriptions in Supporting Information of Deyle et al. (2013) PNAS.
%  25th October 2017.
% sockeye-sim-edm.rnw - converting sockeye-sim-edm.Snw to use knitr. 
%  19th Oct 2017.
% sockeye-sim-edm.Snw. runs salmonTraj() to simulate data and then does EDM 
%  analysis. Using ricker-sim.Snw as a template. 27th September 2017.

% Need to run using  pdflatex  since it needs .pdf figures for the animations.

\documentclass[11pt]{article}   
\input{../../preamble}

\begin{document}

% These were Sweave options, think most can still get used.
% Most useful options (with defaults):
% echo        = TRUE     - includes R code in output file
% keep.source = FALSE    - when echoing, if TRUE then original source is copied to the file, otherwise deparsed source is echoed.
% eval        = TRUE     - if FALSE then chunk is not evaluated
% results     = VERBATIM - R output included verbatim, if TEX output is already proper latex and included as is, 
%                          if HIDE then all output is completely suppressed (but the code executed - good for admb) results options should all be lower case (else get warnings)
% pdf         = TRUE     - whether .pdf figures shall be generated
% eps         = TRUE     - whether .eps figures shall be generated
% strip.white = FALSE    - if true then blank lines at beginning and end of output are removed. If all, then all blank lines are removed.
% width       = 6        - width of figures in inches
% height      = 6        - height of figures in inches
% fig         = FALSE    - whether the code chunk produces graphical output (only one per chunk)

% \setkeys{Gin}{width=6in}     % from googling sweave figure bigger.
%  It will set this for the rest of document [doesn't width do that in the above?]

% <<include=FALSE>>=     was inserted by Sweave2knitr()
<<setupR, echo=FALSE, results='hide', warning=FALSE, comment=NA, message=FALSE>>=
rm(list=ls())
require(rEDM)
require(rgl)                 # for plot3d
require(scatterplot3d)       # for scatterplot3d()
require(dplyr)
require(igraph)              # for curved arrows (igraph.Arrows)
iArrows = igraph:::igraph.Arrows     # Think as it's a hidden function
require(RColorBrewer)        # for brewer.pal
require(corrplot)            # for colorlegend
# require(knitr)             # Need to run before using knit command

source("../../functions.r")
source("../sockeye-simulated/SockeyeSim.r")
figwidth = 5.7 #*1.5  
figheight = 7 # 6 
png.res = 300         # png resolution
# sigma = 0                   # standard deviation of process noise -- needed
                            #  here for headings etc.

# Options for knitr
# cache=TRUE tells knitr to build all the plots into a cache directory
# which is defined here as knitr-cache/
# When the latex part is run, these plots will be inserted automatically.
# If you WANT TO REMAKE THE PLOTS, YOU MUST DELETE THIS DIRECTORY.
# If you don't, the cached plots from last time will be used.
opts_chunk$set(dev = 'pdf', # 'cairo_ps',
               # dev.args=list(type="cairo"),
               fig.path = 'knitr-cache/',
               # fig.dpi = png.res,  # think not to use for .pdf
               # fig.dpi = 96,
               fig.width = figwidth,
               fig.height = figheight,
               fig.align = 'center',
               echo = TRUE,         # same as .Snw was
               results = 'markup',
               comment = NA,
               prompt = TRUE,
               message = FALSE,
               warning = FALSE,
               cache = TRUE,
               cache.path = 'knitr-cache/',
               autodep = TRUE,
               error = FALSE)   # didn't work with knit_exit, though may need
                                #  to update knitr, but keep to exit with error
@ 


\rhead{Manual example using Deyle} 
\pagestyle{fancy}
\thispagestyle{empty}              

\begin{center}
{\LARGE Manual example based on Deyle descriptions\\
  ~\\
  {\tt egDeyle.rnw}}\\
  ~\\
\today{}
\end{center}

Doing manual calculations to help understand exactly what {\tt simplex()} and
{\tt s\_map()} are doing (in particular what the outputs are), based on the
detailed description of techniques in Deyle $et~al.$~(2013) PNAS Supporting
Information. Now using their notation exclusively, in particular ${\bf x}(t)$
for a vector of lagged values, where the lagged values are $X(t), X(t-1),$
etc.

Will stick with simulating Sockeye Salmon spawner abundances from Carrie's 
{\tt salmonTraj()} function, as that was already used in 
{\tt sockeye-sim-edm.rnw} which I'm using as a template. Though here not doing
a full analysis, just trying to reproduce results from the {\tt rEDM} functions.
Will probably remove some of the figures from here that were in 
{\tt sockeye-sim-edm.rnw}.

Let's just pick $E=2$ as can then plot the 2-dimensional points and colour-code
the 3 nearest neighbours.\\

\tableofcontents

\section{Simulate data}

<<simulate>>=
set.seed(42)
T = 100                   # Number of years of simulated data
simulated = salmonTraj(nyears = T+5)   # Need +5 to get T simulated spawners
                         # Simulated annual spawner abundances and recruitments
                         #  (as a list object)
N = simulated$S          # Just the simulated spawners
tvec = 1:T
round(N, 3)
@ 

\section{Want consistent notation}

Sugihara and May (1990) used lagged co-ordinates
$\{x_t, x_{t-\tau}, x_{t-2\tau}, ..., x_{t-(E-1)\tau} \}$, but Deyle et al.
have a notation with these being $\{X(t), X(t-1, ...\}$. 
So each $X(t)$ represents an original scalar value (differenced).

I was going to use that, but is $X_t$ better? I think not as when
you get to $\Xhat$ it's useful to emphasise that the estimate is a
function of $t^*$. Also, you can have $X(t^*)$ and $X(t^*+1)$ but not
necessarily need to think of them as components of a vector. I hadn't
thought of this as a problem until expicitly writing this section on 
notation, so stick with theirs, as I'm used to seeing it anyway. The
code looks the same either way ({\tt X} is a vector in R).
And they do have $x_1(t)$ etc., so $X(t)$ is more
explicitly different. 

The vector in lagged space (with $E=2$) is 
\eb
{\bf x}(t) = \begin{bmatrix}
               x_1(t) \\ x_2(t) 
             \end{bmatrix}
           = \begin{bmatrix}
               X(t) \\ X(t-1)\\
             \end{bmatrix}.
\ee
The $_1$ in $x_1$ just corresponds to that being the first component of {\bf x},
it doesn't represent time (or anything else). 
For multivariate you can have $x_E(t) = Y(t)$ instead of equalling a
lagged value of $X$.

\section{First-difference the data and plot in various ways}

Now to first-difference the data, where the original data are
$N(1), N(2), N(3), ..., N(T)$:
\eb
X(t) = N(t+1) - N(t)
\label{Xt}
\ee
for $t=1, ..., T-1$. Writing it this way, rather than defining
$X(t) = N(t) - N(t-1)$,
means that the vector of values of
$X(t)$ in R is {\tt X} which starts as {\tt X[1]}, instead of
starting with {\tt X[2]} which would get confusing.
<<firstDiff>>=
X = N[-1] - N[-length(N)]    # X = first-difference vector of 
                             #       X(1), X(2), ..., X(T-1)
n = length(X); n
@ 

I don't think you need to standardise the values for a
univariate time series, but you do for multiple time series to get all variables
on the same scale. If standardise then call the standardised values $X(t)$ --
replace (\ref{Xt}) with an intermediary $X'(t)$ value for the unstandardised;
i.e.~$X(t)$ is what we are going to analyse.

<<create.df>>=
# Create Nx.lags with extra columns to represent lagged variables
Nx.lags = data.frame("Nt" = N)            # Nt has length T, time is 1:T
Nx.lags = tbl_df(Nx.lags)
Nx.lags = mutate(Nx.lags,
                   "Ntmin1" = c(NA, Nt[-T]),
                   "Xt" = c(Nt[-1] - Nt[-T], NA),
                   "Xtmin1" = c(NA, Xt[-T]),
                   "Xtmin2" = c(NA, NA, Xt[-c(T-1, T)])
                )     
@ 

<<settings, echo=FALSE, results='hide'>>=
late.num = 3
late.col = "red"
plot.end.N = length(N)
plot.end.X = n
@

\begin{figure}[tp]
<<just-data, echo=FALSE, out.width='110%'>>=
# Use plotPanelMovie.df2() to just plot a modified final panel of the
#  movie (no EDM calcs, no cobwebbing). Keep late.col since useful.
plotPanelMovie.X(Nx.lags = Nx.lags, "knitr-cache/justData.pdf", 
          start = 1,
          end = plot.end.X,
          only.final.plot = TRUE,
          cobwebbing = FALSE,                   
          late.num = late.num, 
          late.col = late.col,
          open.pdf = FALSE
          )
@
\caption{Before doing EDM, this shows the data in various ways:
   time series of $N(t)$ and $X(t)$, 
   plus phase plots of $N(t)$ vs $N(t-1)$, $X(t)$ vs $X(t-1)$ (as used for $E=2$)
   and $X(t)$ vs $X(t-1)$ vs $X(t-2)$ (as used for $E=3$). 
   The last \Sexpr{late.num} points are in \Sexpr{late.col}, with the final
   one as a star.
   In the $X(t)$ time series, the values of $X(t)$ are also plotted in 
   a vertical line (with $t<0$) which is the one-dimensional equivalent of 
   a phase plot for $E=1$. Animated version (with EDM results) is in
   {\tt sockeye-sim-edm.rnw}.}
\end{figure}


\section{Just want full results from {\tt rEDM} for $E=2$}

% So the lagged sequence of data points (from Sugihara and May, 1990) is
% $\{x_t, x_{t-\tau}, x_{t-2\tau}, ..., x_{t-(E-1)\tau} \}$ which for $E=2$ 
% (and $\tau=1$) is just
% $\{x_t, x_{t-1}\}$, and the idea is that you predict $x_{t+1}$.

You need $E$ values to make a point in $E$-dimensional space,
you form the 
simplex of $E+1$ nearest neighbours from the library, and then project ahead
once from ${\bf x}(t^*)$ to estimate where $X(t^*+1)$ will likely end up, and
the prediction is defined as $\Xhat$.

<<Efix>>=
Efix = 2
simp.Efix = simplex(X, E = Efix, stats_only = FALSE)
rho = simp.Efix[[1]]$stats[,"rho"]
simp.Efix     # This is a list with one element (that contains more)
@ 

So, it would be good to manually reproduce (most of) those results, outside of 
the {\tt rEDM} package. In particular the {\tt pred\_var} column.

<<append-res>>=
Nx.lags = mutate(Nx.lags, 
                 XtPredEeq2 = c(NA, simp.Efix[[1]]$model_output$pred))
Nx.lags
@ 

\section{Deyle et al.'s simplex steps (i)-(vii) with code done manually}

Notation (with $E=2$): Time series point $X(t)$. 
Corresponding point in lagged space is
\eb
{\bf x}(t) = \begin{bmatrix}
               x_1(t) \\ x_2(t) 
             \end{bmatrix}
           = \begin{bmatrix}
               X(t) \\ X(t-1)\\
             \end{bmatrix}.
\ee
The $_1$ in $x_1$ just corresponds to that being the first component of {\bf x},
it doesn't represent time (or anything else). 
For multivariate you can have $x_E(t) = Y(t)$ instead of equalling a
lagged value of $X$.

(i) reconstructed state space (i.e.~lagged). Done above as columns of 
{\tt Nx.lags}, and fixed $E=2$.

(ii) pick a target time point $t^*$, identify the corresponding vector
{\bf x}$(t^*)$, and predict $X(t^*+1)$. 

So we know the lagged system 
{\bf x}$(t^*)$ and want to predict the next scalar value $X(t^*+1)$; call the
prediction $\Xhat$.

Fix $t^*$ in the next chunk.
Recall that $t=1, 2, ..., T=\Sexpr{T}$, with corresponding
$N(t)$ but that there is no $X(T) = X(\Sexpr{T})$, because $N(\Sexpr{T+1})$ does 
not exist.

So the corresponding vector {\bf x}$(t^*)$ is
<<X.tstar>>=
tstar = 94                     # the most -ve obs-pred for T=100; 
                               #   also has pred_var = 0 (as turns out it
                               #   only used one nearest neighbor)
X.tstar = as.numeric(select(Nx.lags[tstar,], Xt, Xtmin1))
X.tstar
X.tstar.rEDM = pull(Nx.lags[tstar+1, "XtPredEeq2"])
X.tstar.rEDM                   # X(t*+1) as predicted by rEDM
@ 

(iii) Define set of library vectors that will be used to predict the behaviour of
{\bf x}$(t^*)$. 

We're doing cross-validation so use all  
${\rlap{\rule[0.5ex]{3em}{0.1ex}}\text{vectors}}$~ 
values except the target $X(t^*+1)$.  
Make the target value {\tt NA} in its two occurrences;
it appears as $X(t)$ and as $X(t-1)$:
<<library>>=
library = select(Nx.lags, Xt, Xtmin1)
library[tstar+1, "Xt"] = NA       # This is the value we are trying to predict
library[tstar+2, "Xtmin1"] = NA   #  and therefore is (assumed by rEDM) to be
                                  #  the only thing that we don't know. This
                                  #  is the 'one' in leave-one-out. 
@ 

(iv) Compute the Euclidean distance between {\bf x}$(t^*)$ and each vector in 
the library (and do the ranks here). 

However, I've realised that you can't use a vector that
does not end get projected to a valid vector (i.e.~need to exclude ones for
which the next vector contains {\tt NA}'s, which include {\bf x}$(t^*-1)$ and
{\bf x}$(T-1)$). This should be added to any description of methods that
we write as doesn't seem to be documented (outside of code).

Do distance this as an extra column of {\tt library}, while excluding invalid
ones:
<<euc.distance>>=
library = mutate(library, d=sqrt((Xt - X.tstar[1])^2 + (Xtmin1 - X.tstar[2])^2))
# Any that have d = NA cannot then be the result of a projection:
dIsNA = which(is.na(library$d))
dIsNA = dIsNA[dIsNA > 1]    # >1 as doing dIsNA-1 next, and 1 can't be result of
library[dIsNA-1, "d"] = NA  #  projection anyway
library = mutate(library, rank = dense_rank(d))
@ 
\clearpage               % want to see library on one page, can't find a knitr
                           %  option.
<<library-print>>=
library
@

(v) Identify the $E+1$ library vectors that are closest to target {\bf x}$(t^*)$,
[using $\psi$ instead of the funny $t$ symbol].

<<nearest>>=
psi1 = which(library$rank == 1)    # time index of nearest neighbour
psi2 = which(library$rank == 2)    # time index of second nearest neighbour
psi3 = which(library$rank == 3)    # time index of third nearest neighbour
psivec = c(psi1, psi2, psi3)
psivec
@ 

(vi) Predict $X(t^*+1)$ using equation S1 (setting $p=1$)
\eb
\Xhat | {\bf M}_X = \dfrac{\sum_{i=1}^{E+1} w_i X(\psi_i + 1)}
                             {\sum_{j=1}^{E+1} w_j}
\ee
[defining $\Xhat$ as the predicted value, and setting the sums to $E+1$ (they are
  only to $E$ in the paper, which is incorrect)]
where the weighting, $w_i$, is
\eb
w_i & = & \exp \left( - 
                 \dfrac{\parallel {\bf x}(t^*) - {\bf x}(\psi_i) \parallel}
                   {\parallel {\bf x}(t^*) - {\bf x}(\psi_1) \parallel} \right),
\label{wi}
\ee
or, using the notation that Deyle et al.~(partly) use, and Chang
et al.~kind of do but not as clearly:
\eb
 & = & \exp \left( - 
                  \dfrac{d[{\bf x}(t^*), {\bf x}(\psi_i)]}
                        {d[{\bf x}(t^*), {\bf x}(\psi_1)]} \right),
\ee                    
where
\eb
d[{\bf a}, {\bf b}] & = & \parallel {\bf a} - {\bf b} \parallel\\
 & = & \left[ (a_1 - b_1)^2 + (a_2 - b_2)^2 + ... + (a_E - b_E)^2\right]^{1/2}.
\ee

The numerator in (\ref{wi}) is the $i$th component of the {\tt d} column
in {\tt library} (printed above).

Easy just to compute weights for all vectors, even though only need the first
three. The value of $d[{\bf x}(t^*), {\bf x}(\psi_1)]$ is
<<weights>>=
dxstarpsi1 = as.numeric(library[psi1, "d"])
library = mutate(library, w = exp(- d / dxstarpsi1))
as.data.frame(library)
@ 

Then to predict $X(t^*+1)$ using (equation S1 with $p=1$, but setting 
$E \rightarrow E+1$ as noted above)
\eb
\Xhat = \dfrac{\sum_{i=1}^{E+1} w_i X(\psi_i + 1)}
                             {\sum_{j=1}^{E+1} w_j}
\label{predEqn}
\ee
we just do
<<XtstarPlus1>>=
sumw = sum(library[psivec, "w"])
XtstarPlus1 = sum( library[psivec, "w"] * library[psivec+1, "Xt"] ) / sumw
XtstarPlus1
@ 
which is the predicted value of $X(t^*+1)$. The true value and that estimated
using {\tt rEDM} are
<<XstarPlus1true>>=
Nx.lags[tstar+1, c("Xt", "XtPredEeq2")]
@ 

For $t^*=64$ and $T=100$ {\tt rEDM} does not agree with mine, so spell out the
values in the calculation of $\Xhat$:
<<calc.details>>=
sumw
psivec
pull(library[psivec, "w"])
pull(library[psivec+1, "Xt"])
pull(library[psivec, "w"] * library[psivec+1, "Xt"])
sum(library[psivec, "w"] * library[psivec+1, "Xt"])
sum(library[psivec, "w"] * library[psivec+1, "Xt"]) / sumw
@ 
But (for $t^*=64$ and $T=100$) {\tt rEDM} has given weights of $(1, 0, 0)$,
since this gives the {\tt rEDM} result:
<<rEDM-wrong-weights>>=
sum(c(1, 0, 0) * library[psivec+1, "Xt"]) / 1
X.tstar.rEDM
@ 

Can see there that if the $X(\psi_1+1)$ is an outlying point then $\Xhat$ will
always be pulled towards 0 (I think) because the weight isn't 1, but always
$\mbox{e}^{-1} = 0.3678...$, unless the other neighbours are much further and
so get very low weight. Looks like they got zero weight in {\tt rEDM} calculation
for $t^*=64$. Not yet sure why they get ignored.

Actually may mean that the simplex algorithm does better than suggested by the
{\tt rEDM} results? Since it is predicting $\Xhat$ to be the outlying value which
is a long way from the true value, but doing my averaging I get a value closer
to the true observed value. See Figure~\ref{fig:simplex-animate}.

Other results are also wrong, but $t^*=64$ looks to be the only one that has
{\tt pred\_var}$=0$, and so the other incorrect ones may be incorrect for
different reasons. Look into once get the $t^*$ loop code working.

\subsection{Earlier runs that have .pdf's saved while figuring out some of the code}

Saving these as runs {\tt egDeyleT**tstar**.pdf}:

The prediction isn't great for $T=20$ and $t^*=10$, since a short time
series. Though ....

Also isn't great for $T=95$ and $t^*=10$ -- it's actually because $X(t^*+1)$
is a bit of an outlier, the second highest value. The nearest neighbours are
very close, but actually diverge a bit (even for $T=95$) but are still way
below the true value.

For $T=95$ and $t^*=15$ the prediction is better. All three values head to 
a similar area which is close to the true value.

For $T=20$ and $t^*=15$ the animation shows that only one nearest 
neighbour is being used (saving as {\tt egDeyleT20tstar15preFix.pdf}).
This is because the other two nearest neighbours get projected to invalid
points -- see {\tt egDeyleT20tstar15preFix.pdf} for the explanation.

Have fixed that and saved as {\tt egDeyleT20tstar15preFix2.pdf}, though the
EDM prediction and mine do not yet agree. Saving and printing out to try
and figure out.

{\tt egDeyleT20tstar15preFix3.pdf} -- allows $X(t^*-1)$ to be used in
the projection, but I assumed it shouldn't be because it gets predicted
to $X(t^*)$ {\bf which is what you are trying to predict}, so surely the 
assumption should be that you don't know it. And the method is called 
leave-one-out, though what should be left out is $X(t^*+1)$ not ${\bf x}(t^*)$.
The results for this
example do now agree with the {\tt rEDM} predicted.

{\tt rEDM} correctly does not allow $X(t^*)$ to be used in the prediction of
$X(t^*+1)$, obviously, but this somewhat gets ignored because 
${\bf x}(\psi_1) = {\bf x}(t^*)$, the closest to ${\bf x}(t^*)$ is 
${\bf x}(t^*)$, which results in $d[{\bf x}(t^*), {\bf x}(\psi_1)] = 0$ which
is the denomiator in the weight equation.

So now adjusting the code to just make $X(t^*+1)$ an {\tt NA}, the resulting
{\tt NA} distances (and preceding ones) should then automatically come out
right.

{\tt egDeyleT20tstar15Fix.pdf} -- this fixes the problem, and just sets
$X(t^*) = \mbox{NA}$, and everything else drops out automatically as being
{\tt NA}.

Now to illustrate all this with an animated figure. 

<<simplex-animate, echo=FALSE, results='hide'>>=
Xt.max.abs = max( abs( range(Nx.lags[,"Xt"], na.rm=TRUE) ) )
Xt.axes.range = c(-Xt.max.abs, Xt.max.abs)    # Make axes symmetric
pdf("knitr-cache/simplex-animate.pdf", height = figheight, width = figwidth)
# If adding something once then add it here, if want it for future plots
#  then add it to the function simplexPlot()
# Last one first (and last) so it shows up when opened and printed
simplexPlot(Nx.lags = Nx.lags, library = library, tstar = tstar, 
            main = paste0(
            "and is hopefully close to the true value"),
            tstar.pch = 19, tstar.cex = 1.2,
            neigh.plot = TRUE, psivec = psivec,
            neigh.proj = TRUE, pred.plot = TRUE, pred.rEDM = TRUE,
            true.val = TRUE)

simplexPlot(Nx.lags = Nx.lags, library = library, tstar = tstar,
             tstar.col = "black")

simplexPlot(Nx.lags = Nx.lags, library = library, tstar = tstar,
            main = paste0(
                "Remove vector x(tstar), where tstar=", tstar, ", from library"),
            tstar.pch = 4, tstar.cex = 2.5)

simplexPlot(Nx.lags = Nx.lags, library = library, tstar = tstar, 
            main = paste0(
   "Want to predict where it goes, i.e. predict vec x(tstar+1)"),
            tstar.col = "white")    # to hide it

simplexPlot(Nx.lags = Nx.lags, library = library, tstar = tstar, 
            main = paste0(
            "So predict where vector x(tstar) will go ..."),
            tstar.pch = 19, tstar.cex = 1.2)

simplexPlot(Nx.lags = Nx.lags, library = library, tstar = tstar, 
            main = paste0(
            "... based on three (E+1) nearest neighbours"),
            tstar.pch = 19, tstar.cex = 1.2,
            neigh.plot = TRUE, psivec = psivec)

simplexPlot(Nx.lags = Nx.lags, library = library, tstar = tstar, 
            main = paste0(
            "See where neighbours go"),
            tstar.pch = 19, tstar.cex = 1.2,
            neigh.plot = TRUE, psivec = psivec,
            neigh.proj = TRUE)

simplexPlot(Nx.lags = Nx.lags, library = library, tstar = tstar, 
            main = paste0(
            "and take a weighted average of X(t) to be the predicted value"),
            tstar.pch = 19, tstar.cex = 1.2,
            neigh.plot = TRUE, psivec = psivec,
            neigh.proj = TRUE, pred.plot = TRUE)

simplexPlot(Nx.lags = Nx.lags, library = library, tstar = tstar, 
            main = paste0(
            "which should agree with prediction from rEDM"),
            tstar.pch = 19, tstar.cex = 1.2,
            neigh.plot = TRUE, psivec = psivec,
            neigh.proj = TRUE, pred.plot = TRUE, pred.rEDM = TRUE)

simplexPlot(Nx.lags = Nx.lags, library = library, tstar = tstar, 
            main = paste0(
            "and is hopefully close to the true value"),
            tstar.pch = 19, tstar.cex = 1.2,
            neigh.plot = TRUE, psivec = psivec,
            neigh.proj = TRUE, pred.plot = TRUE, pred.rEDM = TRUE,
            true.val = TRUE)

# Copy the last one to the start so it shows up first when opened and printed
dev.off()
@ 
\begin{figure}
\begin{center}
\animategraphics[controls, width=\linewidth]{2}{knitr-cache/simplex-animate}{}{}
\end{center}
\caption{Animation to show manual calculation of {\tt simplex()} projection,
  for $E=2$, with $t^*=$\Sexpr{tstar} and $T=$\Sexpr{T}. 
  \label{fig:simplex-animate}}
\end{figure}


\section{The predictions are weighted means, now to calculate corresponding 
  variances}

Need to understand the {\tt pred\_var} output values, and reproduce them.
Relevant {\tt C++} code from 
{\tt https://github.com/ha0ye/rEDM/blob/master/src/forecast\_machine.cpp\#L514} 
is (commenting and formatting like R code just to read it), for the mean
<<cpp-mean-code>>=
# predicted[curr_pred] = 0;
#        for(size_t k = 0; k < effective_nn; ++k)
#            predicted[curr_pred] += weights[k] * targets[nearest_neighbors[k]];
#        predicted[curr_pred] = predicted[curr_pred] / total_weight;
@ 
and for variance is
<<cpp-var-code, eval=FALSE>>=
#
# compute variance
#   predicted_var[curr_pred] = 0;
#   for(size_t k = 0; k < effective_nn; ++k)
#         predicted_var[curr_pred] += weights[k] * 
#           pow(targets[nearest_neighbors[k]] - predicted[curr_pred], 2);
#        predicted_var[curr_pred] = predicted_var[curr_pred] / total_weight;
@ 

I think {\tt total\_weight} is just my earlier {\tt sumw}, the denominator of
(\ref{predEqn}). And that the equation for the variance is
\eb
\mbox{Var}(\widehat{X}(t^*+1)) = \dfrac{\sum_{i=1}^{E+1} w_i 
                              \left[ X(\psi_i + 1) - \widehat{X}(t^*+1)\right]^2}
                                   {\sum_{j=1}^{E+1} w_j}
\label{var}
\ee

If correct (testing in code below) then think about this a bit -- is this

(i) a unique definition

(ii) correct -- I kind of think there should be a $w_i^2$ type term maybe?

Actually, (\ref{var}) is the 
biased sample variance (according to Wikipedia site, which will do for now). 
So probably fine to be using.

The {\tt rEDM} variance of $\widehat{X}(t^*+1)$ is
<<rEDM-var>>=
filter(simp.Efix[[1]]$model_output, time == tstar+1)$pred_var
@ 

<<calc-var>>=
XtstarPlus1var = sum( library[psivec, "w"] * 
                         ( library[psivec+1, "Xt"] - XtstarPlus1)^2) / sumw
XtstarPlus1var
@

They clearly do not agree for $t^*=64$; did for another value earlier (can
see better once do the $t^*$ loop).

Want to plot the estimated standard errors (square root of the variances)
that {\tt rEDM} gives to see how they look.
<<stdErr>>=
Efix.points = simp.Efix[[1]]$model_output
Efix.points = tbl_df(Efix.points)
Efix.points = mutate(Efix.points, std_err = sqrt(pred_var))
# min and max but based on std error, not 95% conf intervals
Efix.points = mutate(Efix.points, pred_min = pred - std_err,
                                  pred_max = pred + std_err) 

# mean pred is within the standard errors?
Efix.points = mutate(Efix.points, in_int = 
              ((obs > pred_min) & (obs < pred_max)))
num.in = sum(Efix.points$in_int, na.rm=TRUE)       # within interval
num.poss = sum(!is.na(Efix.points$obs * Efix.points$pred))  # have obs and pred
percent.in = num.in/num.poss * 100

# manually calc rEDM differences
Efix.points = mutate(Efix.points, diff = obs - pred)
@ 

\begin{figure}[tp]
<<errorPlot,  echo=FALSE>>=
yabsmax = max(abs(c(min(Efix.points$pred_min, na.rm=TRUE),
                    max(Efix.points$pred_max, na.rm=TRUE))))
plot(Efix.points$time, Efix.points$obs,
     xlab = "Time", ylab = "Value",
     xlim = c(0, T),
     ylim = c(-yabsmax, yabsmax))
points(Efix.points$time, Efix.points$pred, pch = 19, col = "blue", cex = 0.85)
for(i in 1:nrow(Efix.points))
  {
     lines(c(Efix.points$time[i], Efix.points$time[i]),
           c(Efix.points$pred_min[i], Efix.points$pred_max[i]), 
           col = "blue")
  }
legend("bottomright", legend=c("Observations", "Predictions"),
           pch=c(1, 19), col=c("black", "blue")) # , cex=c(1, 0.85))
@
\caption{Observations and predictions with bars indicating the 
  standard errors based on the computed variance from {\tt rEDM}. Presumably a 
  bigger sample size won't change the errors for any points for which
  the three nearest neighbours are the same. Which is kind of strange?
  In this case \Sexpr{num.in}
  of the \Sexpr{num.poss} (\Sexpr{round(percent.in)}\%) true 
  observations (that have an associated prediction) lie within one standard 
  error of the estimated mean prediction for that observation. For $T=20$ it
  is 3/17 (18\%), $T=50$ is 19/47 (40\%), $T=100$ is 38/97 (39\%), 
  $T=1000$ is 409/997 (41\%), $T=10000$ is 4122/9997 (41\%).}
\end{figure} % $


\subsection{Ideas on improved estimate of uncertainty}

One issue is that the variance is calculate based on only $E+1$ points,
which is not many.

Presumably the uncertainty (be nice to have a confidence interval, though
may have to call it something different) of a prediction $\Xhat$ will depend
upon whereabouts in the attractor the predictee is. I expect there is some way
to use the known uncertainty of how well the nearest neighbours get predicted
(though a nearest neighbour's nearest neighbour may well be itself, so may 
get circular).

In other words the $\rho$ calculation (not yet manually confirmed here, but have
code in earlier work) is for how well the predictions do overall. But some 
predictions will clearly be much better than others depending on where the
neighbours go -- this is captured by $\mbox{Var}(\Xhat)$. It seems like we should
somehow use the $\mbox{Var}(\Xhat)$ values, but may have to modify them to get an
accurate estimate of the uncertainty, ideally as a confidence interval (more
intuitive and easier to simulation test).

The variances are just based on how close the $E+1$ neighbours end up, but 
sometimes this still won't be near the true value. Since we know the true
value (except for, say, the final year that we want to predict), we could modify
each $\mbox{Var}(\Xhat)$, for each $t^*$, based on how well it actually did.

See Figure~\ref{fig:absErrPlot} caption for numbers that fall within 1 
standard error as sample size varies, \emph{based on the {\tt rEDM} 
calculations}. Numbers
to settle down to about 40\% as sample size increases. No real change between
$T=50, 100$ and 1000. So basically more of the attractor gets filled in, 
somewhat proportionally to the increase in sample size. The well-predicted areas
still do well, and the poorly-predicted errors (true obs lies outside 
the predicted mean $\pm$ standard error. 

So for a prediction (say for $X(t^*=T+1)$) you don't want to just use the 
$\mbox{Var}(\Xhat)$ calculation from above -- depending where $X(t^*)$ is you
should be able to work out whether the projection is likely to be very good
or very poor. And thus generate a confidence interval to do that.


Idea: Plot the absolute errors, see how they look. If normal (doubtful) then
you could maybe do confidence intervals based on the standard errors. But we
can probably still get better than that -- the well-sampled areas of the 
attractor can maybe have their confidence intervals reduced.

\begin{figure}[tp]
<<absErrHist>>=
hist(Efix.points$diff, breaks=20)
@
\caption{Histogram of the differences between observed and predicted
  values based on {\tt rEDM} output. Values do not look normaly 
  distributed (for $T=100$).}
\label{fig:absErrHist}
\end{figure}

\begin{figure}[tp]
<<absErrqq>>=
qqnorm(Efix.points$diff, asp=1)
qqline(Efix.points$diff, col="red")
@
\caption{A qqplot of the differences between observed and predicted
  values based on {\tt rEDM} output. Values do not look normaly 
  distributed (for $T=100$).}
\label{fig:absErrqq}
\end{figure}

Result: The absolute errors (observed - predicted from rEDM) 
shown in Figure~\ref{fig:absErrHist} and Figure~\ref{fig:absErrqq}
do not look normally distributed for $t^*=64$ (though based on rEDM output
which may make tails look worse than they are). Had 
expected that they would not (why should they be?). 
This justifies not trying to fit a distribution
(no theoretical justification as what to choose -- looks like a t-distribution
maybe, but no guarantee all examples will be and could end up chasing fits). 


% Colour-coded plot of the points in lagged space, colour coded by the
%  absolute error of how well they are predicted (using leave-one-out). 
% Use rEDM output for now (Efix.points), and do the colours in the dataframe
%  for easier control. Actually, this is a start but Xt and x_t-1 are plotted
%  using Nx.lags, which I'll need anyway at some point, so prob need to change
%  this. Stick with for now.
<<simplex-col-setup>>=
# Manually do colours, divide up the range into num.cols segments
num.cols = 6     # Use even number so 0 is in middle
cols = rev(brewer.pal(num.cols, "Spectral"))
max.diff = ceiling(max(abs(range(Efix.points$diff, na.rm=TRUE))))
                   # ceiling means cut can be simpler
colBreaks = seq(-max.diff, max.diff, length=num.cols+1)  # will have 0
Efix.points$col.bin = cut(Efix.points$diff, 
    breaks = colBreaks, 
    labels=1:num.cols)
@ 


% Crude for now - be careful with Nx.lags having an extra row at start
%  compared to Efix.points. Hence the NA at start of col
\begin{figure}[p]
<<simplex-col, echo=FALSE>>=
par(pty="s")
plot(Nx.lags$Xtmin1, Nx.lags$Xt,
      col = c(NA, cols[Efix.points$col.bin]), 
      pch = 19,
      xlab = expression("x"[t-1]), 
      ylab = expression("x"[t]),
      main = "Coloured by obs-pred of that value, t* is previous index ",
      xlim = Xt.axes.range, ylim = Xt.axes.range,
      asp = 1)
colorlegend(cols, labels = round(colBreaks, 1),
            at = seq(0, 1, len=num.cols+1),
            xlim = c(0.7, 0.9)*Xt.axes.range[2], c(0.2,0.9)*Xt.axes.range[2])
@ 
\caption{Points are observations with colours corresponding to how well the
  $X(t)$ value is predicted by the simplex method (using the {\tt rEDM} output).
  Since predictions are $\Xhat$, here we have $t=t^*-1$.}
\end{figure}

\begin{figure}[p]
<<simplex-col-2, echo=FALSE>>=
# Now colour code by each point being t* - predicting where it goes
par(pty="s")
plot(Nx.lags$Xtmin1, Nx.lags$Xt,
      col = c(cols[Efix.points$col.bin], NA), 
      pch = 19,
      xlab = expression("x"[t-1]), 
      ylab = expression("x"[t]),
      main = "Coloured by obs-pred of next value (point is t*)",
      xlim = Xt.axes.range, ylim = Xt.axes.range, 
      asp = 1)
colorlegend(cols, labels = round(colBreaks, 1),
            at = seq(0, 1, len=num.cols+1),
            xlim = c(0.7, 0.9)*Xt.axes.range[2], c(0.2,0.9)*Xt.axes.range[2])
@   
\caption{Points are observations with colours corresponding to how well 
  predicted the $X(t+1)$ value is when $X(t)$ is left out; 
  results from the simplex method (using the {\tt rEDM} output).
  Since predictions are $\Xhat$, here we have $t=t^*$.}
\end{figure}

For $t^*=64$ and $T=100$, the prediction from {\tt rEDM} does not agree with 
my manual one -- see the animation. Have now found the other values that this
happens when doing {\tt tStarLoop.rnw}. From that we have the values 
$t^* = 18, 22, 64, 75, 89, 94, 97$. 

So re-running this code with each of those values, and saving as 
{\tt egDeyleAtstarXX.pdf}, where XX is $t^*$. Write up the results
and conclusions of this in {\tt tStarLoop.rnw} rather than appending here.

\end{document}
